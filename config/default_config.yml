streams_directory: "./config/streams/era5_1deg/"
# streams_directory: "./config/streams/era5_nppatms_synop/"

### Model parameters ###

model :
  embedding :
      #
      embed :
          orientation: "channels"
          unembed_mode: "block"
          dropout_rate: 0.1
      #     
      local :
          dim_embed: 1024
          num_blocks: 2 
          local_num_heads: 16
          dropout_rate: 0.1
          with_qk_lnorm: True
      # 
      adapter :
          num_queries: 1               # Remove?
          queries_per_cell : False     # Remove?
          num_heads : 16
          embed : 128
          with_residual : True
          with_qk_lnorm: True
          dropout_rate: 0.1
      #
      global :
        dim_embed : 2048 
        num_blocks : 8
        num_heads : 32
        dropout_rate : 0.1
        with_qk_lnorm: True
        att_dense_rate: 1.0
        block_factor: 64
        mlp_hidden_factor: 2

  forecast_engine:
    pass
    # type : deterministic
    # blocks: 6
    # dropout_rate : 0.1

  decoder :
      type : PerceiverIOCoordConditioning
      adapter_kv: False
      self_attention: True
      dyadic_dims: False
      mlp_adaln: True
      target_cell_local_prediction: True       # Remove?

  # a regex that needs to fully match the name of the modules you want to freeze
  # e.g. ".*ERA5" will match any module whose name ends in ERA5\
  # encoders and decoders that exist per stream have the stream name attached at the end
  freeze_modules: ""


forecast :
    model : deterministic
    num_blocks: 0
    num_heads: 16
    dropout_rate: 0.1
    with_qk_lnorm: True

### Learning rate params ###

learning_rate :
    scaling_policy: "sqrt"
    start: 1e-6
    max: 5e-5
    final_decay: 1e-6
    final: 0.0
    steps_warmup: 512 
    steps_cooldown: 512
    policy_warmup: "cosine"
    policy_decay: "constant"
    policy_cooldown: "linear"


### Shared model+training parameters ###
# TODO: rename

shared_params :
    with_mixed_precision: True
    with_flash_attention: True
    compile_model: False
    with_fsdp: True
    attention_dtype: bf16
    mlp_norm_eps: 1e-5
    norm_eps: 1e-4
    grad_clip: 1.0
    weight_decay: 0.1
    norm_type: "LayerNorm"
    nn_module: "te"
    log_grad_norms: False

### Latent noising parameters ###

latent_noise :
    kl_weight : 0.0 # 1e-5
    gamma : 2.0
    saturate_encodings : 5 
    use_additive_noise : False
    deterministic_latents : True

### Training parameters ###

training_strategy :
  # masking, forecasting, student-teacher
  mode : "student-teacher"
  # 
  source : 
    - masking_params : 
        strategy : "healpix"
        num_samples : 4
        rate : 0.4
        hl_mask: 4 
        same_strategy_per_batch: False
        teacher_relationship: subset
       
    - masking_params : 
        strategy : "random"
        num_samples : 4
        rate : 0.4
        hl_mask: 4 
        same_strategy_per_batch: False
        teacher_relationship: subset


 # ignored depending on the training mode
 # invalid syntax
  target :
    masking_params : 
      target_aux: ema_teacher
      strategy : "healpix"
      num_samples : 2
      rate : 0.8
      hl_mask: 1 
      same_strategy_per_batch: False
      teacher_relationship: subset
      loss: DINOLoss, JEPALoss
    strategy : 
      - loss: JEPA
    losses : { 
      LossLatentSSLStudentTeacher: { 
        "iBOT": {'weight': 0.5, "loss_extra_args": { "student_temp": 0.1,},"out_dim": 65536, "n_register_tokens": 4,"teacher_temp": 0.1,
            "teacher_style": "softmax_center", "center_momentum": 0.9}, 
        "DINO": {'weight': 0.5, "loss_extra_args": { "student_temp": 0.1,}, "out_dim": 65536, "n_register_tokens": 4, "teacher_temp": 0.1,
            "teacher_style": "softmax_center", "center_momentum": 0.9}, 
        "JEPA": {'weight': 0.5, "loss_extra_args": {}, "out_dim": 2048, "n_register_tokens": 4} }
    }


# training_strategy :
#   # masking, forecasting, student-teacher
#   mode : "masking"
#   # 
#   source : 
#     num_samples : 4
#     masking_rate : 0.5
#     source_params: 
#       # will be used with masking is moved under here
#       masking_strategy: "random"
#       probabilities: [0.34, 0.33, 0.33]
#       hl_mask: 0 
#       mode: per_cell
#       same_strategy_per_batch: False
#                     
#   # ignored depending on the training mode
#   target :
#     - target_aux: physical
#       loss : physical
#     #- target_aux: encoder
#     #  loss: latent_mae
#     #- target_aux: EMATeacher
#     # prediction_head: DINOv2
#     #  centering: 0.5
#     #  loss: latent_mae

#training_strategy :
#  # masking, forecasting, student-teacher
#  mode : "forecasting"
#  # 
#  source : 
#    num_samples : 4
#    source_params: 
#      impute_latent_noise_std: 0.
#      forecast_offset : 1
#      forecast_delta_hrs: 0
#      forecast_steps: 0
#      forecast_with_step_conditioning: False
#      impute_latent_noise_std: 0.0  # 1e-4#

#  # ignored depending on the training mode
#  target :
#    target_aux: physical

### Data parameters ###
data :
    # start_date: 197901010000
    start_date: 201401010000
    end_date: 202012310000
    start_date_val: 202101010000
    end_date_val: 202201010000
    len_hrs: 6
    step_hrs: 6
    input_window_steps: 1
    samples_per_epoch: 4096
    samples_per_validation: 512
    shuffle: True

### Logging params ###
#train_log_freq_params:
#  terminal: 10
#  metrics: 20
#  checkpoint: 250
#  log_validation: 0


### TODO place these ###

misc: 
  num_epochs: 32
  val_initial: False
  loader_num_workers: 8
  analysis_streams_output: ["ERA5"]
  run_history: []
  istep: 0
  desc: ""
  data_loader_rng_seed: ???
  run_id: ???


########################################################################################################
# OLD CONFIG BELOW
#######################################################################################################

model_input:
  masking_strategy: "healpix" # "random", "healpix". Masking strategy to use for model input for masking, and local (student) views when doing student-teacher
  rate: 0.5  # Masking rate to use for model input
  num_views: 1  # if student-teacher, the number of local (student) views to generate
  masking_strategy_config: {"strategies": ["random", "healpix", "channel"], # will be used with masking is moved under here
                            "probabilities": [0.34, 0.33, 0.33],
                            "hl_mask": 0, "mode": "per_cell",
                            "same_strategy_per_batch": false
                            }
  relationship: "subset"  # "independent", "subset", "disjoint". Relationship of student views to teacher view.

teacher_model_input:
  strategy: "healpix"  # Strategy for teacher (global) view: "random", "healpix"
  rate: 0.5  # Fraction of data to keep in global view (alternative: use "keep_m" for absolute count)
  # keep_m: 100  # Alternative to rate: keep exactly this many parent cells
  rate_sampling: true  # randomly sample the rate per batch
  masking_strategy_config: {"strategies": ["random", "healpix", "channel"], 
                            "probabilities": [0.34, 0.33, 0.33],
                            "hl_mask": 4, "mode": "per_cell",
                            "same_strategy_per_batch": false
                            }


embed_orientation: "channels"
embed_unembed_mode: "block"
embed_dropout_rate: 0.1

target_cell_local_prediction: True

ae_local_dim_embed: 256
ae_local_num_blocks: 2
ae_local_num_heads: 16
ae_local_dropout_rate: 0.1
ae_local_with_qk_lnorm: True

ae_local_num_queries: 1
ae_local_queries_per_cell: False
ae_adapter_num_heads: 16
ae_adapter_embed: 128
ae_adapter_with_qk_lnorm: True
ae_adapter_with_residual: True
ae_adapter_dropout_rate: 0.1

ae_global_dim_embed: 512
ae_global_num_blocks: 8
ae_global_num_heads: 32
ae_global_dropout_rate: 0.1
ae_global_with_qk_lnorm: True
# TODO: switching to < 1 triggers triton-related issues.
# See https://github.com/ecmwf/WeatherGenerator/issues/1050
ae_global_att_dense_rate: 1.0
ae_global_block_factor: 64
ae_global_mlp_hidden_factor: 2

decoder_type: PerceiverIOCoordConditioning # CrossAttentionAdaNormConditioning
pred_adapter_kv: False
pred_self_attention: True
pred_dyadic_dims: False
pred_mlp_adaln: True

# number of steps offset applied to first target window; if set to zero and forecast_steps=0 then
# one is training an auto-encoder
forecast_offset : 0
forecast_delta_hrs: 0
forecast_steps: 0
forecast_policy: null
forecast_att_dense_rate: 1.0
forecast_with_step_conditioning: False
fe_num_blocks: 0
fe_num_heads: 16
fe_dropout_rate: 0.1
fe_with_qk_lnorm: True
impute_latent_noise_std: 0.0  # 1e-4

healpix_level: 5

with_mixed_precision: True
with_flash_attention: True
compile_model: False
with_fsdp: True
attention_dtype: bf16
mlp_norm_eps: 1e-5
norm_eps: 1e-4

latent_noise_kl_weight: 0.0 # 1e-5
latent_noise_gamma: 2.0
latent_noise_saturate_encodings: 5 
latent_noise_use_additive_noise: False
latent_noise_deterministic_latents: True 

batch_size_per_gpu: 1
batch_size_validation_per_gpu: 1

# a regex that needs to fully match the name of the modules you want to freeze
# e.g. ".*ERA5" will match any module whose name ends in ERA5\
# encoders and decoders that exist per stream have the stream name attached at the end
freeze_modules: ""

# whether to track the exponential moving average of weights for validation
validate_with_ema: True
ema_ramp_up_ratio: 0.09
ema_halflife_in_thousands: 1e-3

# training mode: "forecast" or "masking" (masked token modeling)
# for "masking" to train with auto-encoder mode, forecast_offset should be 0
training_mode: "masking"
target_and_aux_calc: "EMATeacher"
training_mode_config: {"losses": {LossLatentSSLStudentTeacher: {
        "weight": 1.0,
        "iBOT": {'weight': 0.5, "loss_extra_args": { "student_temp": 0.1,},"out_dim": 65536, "n_register_tokens": 4,"teacher_temp": 0.1,
            "teacher_style": "softmax_center", "center_momentum": 0.9}, 
        "DINO": {'weight': 0.5, "loss_extra_args": { "student_temp": 0.1,}, "out_dim": 65536, "n_register_tokens": 4, "teacher_temp": 0.1,
            "teacher_style": "softmax_center", "center_momentum": 0.9}, 
        "JEPA": {'weight': 0.5, "loss_extra_args": {}, "out_dim": 2048, "n_register_tokens": 4} }
    }}
validation_mode_config: {"losses": {LossPhysical: {weight: 1.0, loss_fcts: [['mse', 1.0]]},}
                        }

# masking
masking_strategy: "dog" # obviously TODO
# masking rate when training mode is "masking"; ignored in foreacast mode
masking_rate: 0.6
#
sampling_rate_target: 1.0
# sample the masking rate (with normal distribution centered at masking_rate)
# note that a sampled masking rate leads to varying requirements
masking_rate_sampling: True
# masking_strategy_config is a dictionary of additional parameters for the masking strategy
# required for "healpix" and "channel" masking strategies
# "healpix": requires healpix mask level to be specified with `hl_mask`
# "channel": requires "mode" to be specified, "per_cell" or "global",
masking_strategy_config: {"strategies": ["random", "healpix", "channel"], 
                          "probabilities": [0.34, 0.33, 0.33],
                          "hl_mask": 3, "mode": "per_cell",
                          "same_strategy_per_batch": false
                          }

# Student-teacher configuration (only used when training_mode == "student_teacher")
# TODO: adapt so that the masking or forecast config entry also sits here
training_config:
  # when this is "masking", we are basically only using the model_input subconfig
  training_mode: "student_teacher"  # "masking", "student_teacher", "forecast"


  model_input:
    masking_strategy: "healpix" # "random", "healpix". Masking strategy to use for model input for masking, and local (student) views when doing student-teacher
    rate: 0.4  # Masking rate to use for model input
    num_views: 4  # if student-teacher, the number of local (student) views to generate
    hl_mask : 4  # healpix level to use for healpix masking strategy
    relationship: "subset"  # "independent", "subset", "disjoint". Relationship of student views to teacher view.
  
  teacher_model_input:
    strategy: "healpix"  # Strategy for teacher (global) view: "random", "healpix"
    rate: 0.8  # Fraction of data to keep in global view (alternative: use "keep_m" for absolute count)
    num_views: 2 # number of teacher views to generate
    hl_mask : 0 # healpix level to use for healpix masking strategy
    # keep_m: 100  # Alternative to rate: keep exactly this many parent cells
    rate_sampling: true  # randomly sample the rate per batch



num_mini_epochs: 32
samples_per_mini_epoch: 4096
samples_per_validation: 512

shuffle: True

lr_scaling_policy: "sqrt"
lr_start: 1e-6
lr_max: 5e-5
lr_final_decay: 1e-6
lr_final: 0.0
lr_steps_warmup: 512 
lr_steps_cooldown: 512
lr_policy_warmup: "cosine"
lr_policy_decay: "constant"
lr_policy_cooldown: "linear"

grad_clip: 1.0
weight_decay: 0.1
norm_type: "LayerNorm"
nn_module: "te"
log_grad_norms: False

# start_date: 197901010000
start_date: 201401010000
end_date: 202012310000
start_date_val: 202101010000
end_date_val: 202201010000
len_hrs: 6
step_hrs: 6
input_window_steps: 1

val_initial: False

loader_num_workers: 0
log_validation: 0
streams_output: ["ERA5"]

istep: 0
run_history: []

desc: ""
data_loader_rng_seed: ???
run_id: ???

# The period to log in the training loop (in number of batch steps)
train_log_freq:
  terminal: 10
  metrics: 20
  checkpoint: 250

# #################
# ###    Data   ###
# #################
# streams_directory: "./config/streams/era5_1deg/"
# 
# start_date: 197901010000
# end_date: 202012310000
# start_date_val: 202101010000
# end_date_val: 202201010000
# len_hrs: 6
# step_hrs: 6
# input_window_steps: 1
# 
# val_initial: False
# 
# loader_num_workers: 8
# log_validation: 0
# analysis_streams_output: ["ERA5"]
# 
# #################
# ###   Model   ###
# #################
# embed_orientation: "channels"
# embed_unembed_mode: "block"
# embed_dropout_rate: 0.1
# 
# target_cell_local_prediction: True
# 
# ae_local_dim_embed: 1024
# ae_local_num_blocks: 2
# ae_local_num_heads: 16
# ae_local_dropout_rate: 0.1
# ae_local_with_qk_lnorm: True
# 
# ae_local_num_queries: 1
# ae_local_queries_per_cell: False
# ae_adapter_num_heads: 16
# ae_adapter_embed: 128
# ae_adapter_with_qk_lnorm: True
# ae_adapter_with_residual: True
# ae_adapter_dropout_rate: 0.1
# 
# ae_global_dim_embed: 2048
# ae_global_num_blocks: 8
# ae_global_num_heads: 32
# ae_global_dropout_rate: 0.1
# ae_global_with_qk_lnorm: True
# # TODO: switching to < 1 triggers triton-related issues.
# # See https://github.com/ecmwf/WeatherGenerator/issues/1050
# ae_global_att_dense_rate: 1.0
# ae_global_block_factor: 64
# ae_global_mlp_hidden_factor: 2
# 
# decoder_type: PerceiverIOCoordConditioning # CrossAttentionAdaNormConditioning
# pred_adapter_kv: False
# pred_self_attention: True
# pred_dyadic_dims: False
# pred_mlp_adaln: True
# 
# healpix_level: 5
# 
# latent_noise_kl_weight: 0.0 # 1e-5
# latent_noise_gamma: 2.0
# latent_noise_saturate_encodings: 5 
# latent_noise_use_additive_noise: False
# latent_noise_deterministic_latents: True 
# 
# #################
# ###  Forecast ###
# #################
# # number of steps offset applied to first target window; if set to zero and forecast_steps=0 then
# # one is training an auto-encoder
# forecast_offset : 0
# forecast_delta_hrs: 0
# forecast_steps: 0
# forecast_policy: null
# forecast_att_dense_rate: 1.0
# forecast_with_step_conditioning: False
# fe_num_blocks: 0
# fe_num_heads: 16
# fe_dropout_rate: 0.1
# fe_with_qk_lnorm: True
# impute_latent_noise_std: 0.0  # 1e-4
# 
# #################
# ###  Training ###
# #################
# loss_fcts:
#   - 
#     - "mse"
#     - 1.0
# loss_fcts_val:
#   -
#     - "mse"
#     - 1.0
# 
# 
# batch_size_per_gpu: 1
# batch_size_validation_per_gpu: 1
# 
# # a regex that needs to fully match the name of the modules you want to freeze
# # e.g. ".*ERA5" will match any module whose name ends in ERA5\
# # encoders and decoders that exist per stream have the stream name attached at the end
# freeze_modules: ""
# 
# # whether to track the exponential moving average of weights for validation
# validate_with_ema: True
# ema_ramp_up_ratio: 0.09
# ema_halflife_in_thousands: 1e-3
# 
# # training mode: "forecast" or "masking" (masked token modeling) or "student-teacher"
# # for "masking" to train with auto-encoder mode, forecast_offset should be 0
# training_mode: "masking"
# training_mode_config: {
#   "losses" : { LossPhysical: {weight: 1.0, loss_fcts: [['mse', 1.0]]}},
#     # LossLatentSSLStudentTeacher: { 
#     #   "iBOT": {'weight': 0.5, "out_dim": 65536, "n_register_tokens": 4, "student_temp": 0.1,"teacher_temp": 0.1,
#     #       "teacher_style": "softmax_center", "center_momentum": 0.9}, 
#     #   "DINO": {'weight': 0.5, "out_dim": 65536, "n_register_tokens": 4, "student_temp": 0.1,"teacher_temp": 0.1,
#     #       "teacher_style": "softmax_center", "center_momentum": 0.9}, 
#     #   "JEPA": {'weight': 0.5, "out_dim": 2048, "n_register_tokens": 4} } }, 
#   # "target_and_aux_calc": "EMATeacher",
#   "target_and_aux_calc": "identity",
#   "teacher_model": {}
# }
# validation_mode_config: {"losses": {LossPhysical: {weight: 1.0, loss_fcts: [['mse', 1.0]],}}}
# 
# # masking
# masking_strategy: "random" # obviously TODO
# # masking rate when training mode is "masking"; ignored in foreacast mode
# masking_rate: 0.6
# #
# sampling_rate_target: 1.0
# # sample the masking rate (with normal distribution centered at masking_rate)
# # note that a sampled masking rate leads to varying requirements
# masking_rate_sampling: True
# # masking_strategy_config is a dictionary of additional parameters for the masking strategy
# # required for "healpix" and "channel" masking strategies
# # "healpix": requires healpix mask level to be specified with `hl_mask`
# # "channel": requires "mode" to be specified, "per_cell" or "global",
# masking_strategy_config: {"strategies": ["random", "healpix", "channel"], 
#                           "probabilities": [0.34, 0.33, 0.33],
#                           "hl_mask": 3, "mode": "per_cell",
#                           "same_strategy_per_batch": false
#                           }
# 
# #################
# ###  Trainer  ###
# #################
# with_mixed_precision: True
# with_flash_attention: True
# compile_model: False
# with_fsdp: True
# attention_dtype: bf16
# mlp_norm_eps: 1e-5
# norm_eps: 1e-4
# # Student-teacher configuration (only used when training_mode == "student_teacher")
# # TODO: adapt so that the masking or forecast config entry also sits here
# training_config:
#   # when this is "masking", we are basically only using the model_input subconfig
#   training_mode: "student_teacher"  # "masking", "student_teacher", "forecast"
# 
# 
#   model_input:
#     masking_strategy: "healpix" # "random", "healpix". Masking strategy to use for model input for masking, and local (student) views when doing student-teacher
#     rate: 0.5  # Masking rate to use for model input
#     num_views: 4  # if student-teacher, the number of local (student) views to generate
#     masking_strategy_config: {"strategies": ["random", "healpix", "channel"], # will be used with masking is moved under here
#                               "probabilities": [0.34, 0.33, 0.33],
#                               "hl_mask": 0, "mode": "per_cell",
#                               "same_strategy_per_batch": false
#                               }
#     relationship: "subset"  # "independent", "subset", "disjoint". Relationship of student views to teacher view.
#   
#   teacher_model_input:
#     strategy: "healpix"  # Strategy for teacher (global) view: "random", "healpix"
#     rate: 0.5  # Fraction of data to keep in global view (alternative: use "keep_m" for absolute count)
#     num_views: 2 # number of teacher views to generate
#     # keep_m: 100  # Alternative to rate: keep exactly this many parent cells
#     rate_sampling: true  # randomly sample the rate per batch
#     masking_strategy_config: {"strategies": ["random", "healpix", "channel"], 
#                               "probabilities": [0.34, 0.33, 0.33],
#                               "hl_mask": 4, "mode": "per_cell",
#                               "same_strategy_per_batch": false
#                               }
# 
# 
# 
# num_mini_epochs: 32
# samples_per_mini_epoch: 4096
# samples_per_validation: 512
# 
# shuffle: True
# 
# lr_scaling_policy: "sqrt"
# lr_start: 1e-6
# lr_max: 5e-5
# lr_final_decay: 1e-6
# lr_final: 0.0
# lr_steps_warmup: 512 
# lr_steps_cooldown: 512
# lr_policy_warmup: "cosine"
# lr_policy_decay: "constant"
# lr_policy_cooldown: "linear"
# 
# grad_clip: 1.0
# weight_decay: 0.1
# norm_type: "LayerNorm"
# nn_module: "te"
# log_grad_norms: False
# 
# istep: 0
# run_history: []
# 
# desc: ""
# data_loader_rng_seed: ???
# run_id: ???
# 
# # The period to log in the training loop (in number of batch steps)
# train_log_freq:
#   terminal: 10
#   metrics: 20
#   checkpoint: 250
# 
# log_level: DEBUG
